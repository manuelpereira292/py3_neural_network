{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isto importa numpy, que é uma biblioteca de álgebra linear.\n",
    "# Esta é a nossa única dependência.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta é a nossa \"não linearidade\".\n",
    "# Embora possa haver vários tipos de funções, essa não linearidade mapeia uma função chamada \"sigmóide\".\n",
    "# Uma função sigmóide mapeia qualquer valor para um valor entre 0 e 1.\n",
    "# Nós a usamos para converter números em probabilidades.\n",
    "# Ele também possui várias outras propriedades desejáveis para o treinamento de redes neurais.\n",
    "def nonlin(x, deriv=False):\n",
    "    # Observe que essa função também pode gerar a derivada de um sigmóide (quando deriv = True).\n",
    "    # Uma das propriedades desejáveis de uma função sigmóide é que sua saída pode ser usada para criar sua derivada.\n",
    "    # Se a saída do sigmoide for uma variável \"out\", a derivada será simplesmente out * (1-out). Isto é muito eficiente.\n",
    "    # Se você não é familiar com derivadas, pense nisso como a inclinação da função sigmóide em um determinado ponto.\n",
    "    if deriv == True:\n",
    "        return x * (1 - x)\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isso inicializa nosso conjunto de dados de entrada como uma matriz numpy.\n",
    "# Cada linha é um único \"exemplo de treinamento\".\n",
    "# Cada coluna corresponde a um dos nossos nós de entrada.\n",
    "# Assim, temos 3 nós de entrada na rede e 4 exemplos de treinamento.\n",
    "X = np.array([[0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isso inicializa nosso conjunto de dados de saída.\n",
    "# Nesse caso, gerei o conjunto de dados horizontalmente (com uma única linha e 4 colunas) para espaço.\n",
    "# \".T\" é a função de transposição.\n",
    "# Após a transposição, essa matriz y possui 4 linhas com uma coluna.\n",
    "# Assim como nossa entrada, cada linha é um exemplo de treinamento e cada coluna (apenas uma) é um nó de saída.\n",
    "# Portanto, nossa rede possui 3 entradas e 1 saída.        \n",
    "Y = np.array([[0,0,1,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# É uma boa prática propagar seus números aleatórios.\n",
    "# Seus números ainda serão distribuídos aleatoriamente,\n",
    "# mas serão distribuídos aleatoriamente exatamente da mesma maneira sempre que você treinar.\n",
    "# Isso facilita ver como suas alterações afetam a rede.\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta é a nossa matriz de peso para esta rede neural.\n",
    "# É chamado \"syn0\" para implicar \"sinapse zero\".\n",
    "# Como temos apenas 2 camadas (entrada e saída), precisamos apenas de uma matriz de pesos para conectá-las.\n",
    "# Sua dimensão é (3,1) porque temos 3 entradas e 1 saída.\n",
    "# Outra maneira de ver é que l0 é do tamanho 3 e l1 é do tamanho 1.\n",
    "# Assim, queremos conectar todos os nós em l0 a todos os nós em l1,\n",
    "# o que requer uma matriz de dimensionalidade (3,1).\n",
    "\n",
    "# Observe também que ele é inicializado aleatoriamente com uma média de zero.\n",
    "# Há um pouco de teoria que entra na inicialização do peso.\n",
    "# Por enquanto, considere como prática recomendada que seja uma boa idéia ter uma média de zero na inicialização do peso.\n",
    "\n",
    "# Outra observação é que a \"rede neural\" é realmente apenas essa matriz.\n",
    "# Temos \"camadas\" l0 e l1, mas são valores transitórios com base no conjunto de dados.\n",
    "# Nós não os salvamos. Todo o aprendizado é armazenado na matriz syn0.\n",
    "syn0 = 2 * np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isso inicia nosso código de treinamento de rede real.\n",
    "# O loop for \"itera\" várias vezes no código de treinamento para otimizar nossa rede para o conjunto de dados.\n",
    "for iter in range(10000):\n",
    "    # Desde a nossa primeira camada \"X\", são simplesmente nossos dados.\n",
    "    # Descrevemos explicitamente como tal neste momento.\n",
    "    # Lembre-se de que X contém 4 exemplos de treinamento (linhas).\n",
    "    # Vamos processar todos eles ao mesmo tempo nesta implementação.\n",
    "    # Isso é conhecido como treinamento em \"lote inteiro\".\n",
    "    # Portanto, temos 4 linhas diferentes de X, mas você pode pensar nisso como um único exemplo de treinamento, se desejar.\n",
    "    # Não faz diferença neste momento. (Poderíamos carregar 1000 ou 10.000, se quiséssemos, sem alterar nenhum código).\n",
    "    l0 = X\n",
    "    # Este é o nosso passo de previsão.\n",
    "    # Basicamente, primeiro deixamos a rede \"tentar\" prever a saída dada a entrada.\n",
    "    # Em seguida, estudaremos o desempenho, para que possamos ajustá-lo para melhorar um pouco a cada iteração.\n",
    "    # Esta linha contém 2 etapas. A primeira matriz multiplica 10 por syn0.\n",
    "    # O segundo passa nossa saída pela função sigmóide.\n",
    "    # Considere as dimensões de cada um:\n",
    "    # (4 x 3) ponto (3 x 1) = (4 x 1)\n",
    "    # A multiplicação da matriz é ordenada, de modo que as dimensões no meio da equação devem ser as mesmas.\n",
    "    # A matriz final gerada é, portanto, o número de linhas da primeira matriz e o número de colunas da segunda matriz.\n",
    "    # Como carregamos 4 exemplos de treinamento, terminamos com 4 palpites para a resposta correta, uma matriz (4 x 1).\n",
    "    # Cada saída corresponde ao palpite da rede para uma determinada entrada.\n",
    "    # Talvez se torne intuitivo o motivo pelo qual poderíamos \"carregar\" um número arbitrário de exemplos de treinamento.\n",
    "    # A multiplicação da matriz ainda funcionaria.\n",
    "    l1 = nonlin(np.dot(l0,syn0))\n",
    "    # Portanto, dado que l1 tinha um \"palpite\" para cada entrada.\n",
    "    # Agora podemos comparar quão bem ele foi subtraindo a resposta verdadeira (Y) da suposição (l1).\n",
    "    # l1_error é apenas um vetor de números positivos e negativos que refletem o quanto a rede perdeu.\n",
    "    l1_error = Y - l1\n",
    "    # Agora estamos chegando às coisas boas!\n",
    "    # Este é o molho secreto!\n",
    "    # Há muita coisa acontecendo nessa linha, então vamos dividir ainda mais em duas partes.\n",
    "    \n",
    "    # nonlin(l1,True)\n",
    "    # Se l1 representa esses três pontos, o código acima gera as inclinações das linhas abaixo.\n",
    "    # Observe que valores muito altos, como x = 2,0 (ponto verde), e valores muito baixos, como x = -1,0 (ponto roxo) têm inclinações bastante rasas.\n",
    "    # A inclinação mais alta que você pode ter é x = 0 (ponto azul). Isso desempenha um papel importante.\n",
    "    # Observe também que todos os derivativos estão entre 0 e 1.\n",
    "    \n",
    "    # Existem maneiras mais \"matematicamente precisas\" do que \"a derivada ponderada pelo erro\", mas acho que isso captura a intuição.\n",
    "    # l1_error é uma matriz (4,1). nonlin (l1, True) retorna uma matriz (4,1). O que estamos fazendo é multiplicá-los \"elementwise\".\n",
    "    # Isso retorna uma matriz (4,1) l1_delta com os valores multiplicados.\n",
    "    # Quando multiplicamos as \"inclinações\" pelo erro, estamos reduzindo o erro de previsões de alta confiança.\n",
    "    # Veja a imagem sigmóide novamente! Se a inclinação era realmente rasa (perto de 0), a rede tinha um valor muito alto ou muito baixo.\n",
    "    # Isso significa que a rede estava bastante confiante de uma maneira ou de outra.\n",
    "    # No entanto, se a rede adivinhou algo próximo a (x = 0, y = 0,5), não ficou muito confiante.\n",
    "    # Atualizamos essas previsões \"insolentes\" com mais intensidade e tendemos a deixar as confiantes em paz, multiplicando-as por um número próximo a 0.\n",
    "    l1_delta = l1_error * nonlin(l1,True)\n",
    "    # Agora estamos prontos para atualizar nossa rede! Vamos dar uma olhada em um único exemplo de treinamento.\n",
    "    \n",
    "    syn0 += np.dot(l0.T,l1_delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Output After Training:\n[[0.00966449]\n [0.00786506]\n [0.99358898]\n [0.99211957]]\n"
    }
   ],
   "source": [
    "print(\"Output After Training:\")\n",
    "print(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 9.67299303],\n       [-0.2078435 ],\n       [-4.62963669]])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitb68cd10d1ee049cfb24fba5da657b80d",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}